---
title: "EDS 232 - Lab 2a Clustering and Lab 2b Ordination"
author: "Steven Cognac"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_float: true
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, tibble)

# set seed for reproducible results
set.seed(42)
```

# 1 Clustering
Clustering associates similar data points with each other, adding a grouping label. It is a form of unsupervised learning since we don’t fit the model based on feeding it a labeled response (i.e. y).

## 1.1 K-Means Clustering

Source:[K Means Clustering in R | DataScience+](https://datascienceplus.com/k-means-clustering-in-r/)\
\
In k-means clustering, the number of clusters needs to be specified. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps:\
  
1. Reassign data points to the cluster whose centroid is closest.\
2. Calculate new centroid of each cluster.\
  
These two steps are repeated until the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids.\
  
### 1.1.1 Load and plot the `iris` dataset
```{r}
# load the dataset
data("iris")

# look at documentation in RStudio
if (interactive())
  help(iris)

# show data table
datatable(iris)

# plot petal length vs width, species naive
ggplot(
  iris, aes(Petal.Length, Petal.Width)) +
  geom_point()

# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  iris, aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point() +
  legend_pos
```

### 1.1.2 Cluster `iris` using `kmeans()`
```{r}
# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# show cluster result
iris_k

# compare clusters with species (which were not used to cluster)
table(iris_k$cluster, iris$Species)
```
**Question: How many observations could be considered “mis-classified” if expecting petal length and width to differentiate between species?**\
 - Based on the original `iris` dataset, there are a total of 150 observations with 50 datapoints for each species. With the k-means classification when $k=3$, there is 6 datapoints that could be considered "mis-classified". 

```{r}
# species classification totals
iris_tot <- iris %>%
  count(Species)
iris_tot

# k-means classification totals
iris_k_tot <- iris_k[["size"]]
iris_k_tot 
```


```{r}
# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos
```
### 1.1.3 Plot Voronoi diagram of clustered `iris`

This form of clustering assigns points to the cluster based on nearest centroid. You can see the breaks more clearly with a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).
```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
box <- tribble(
  ~Petal.Length, ~Petal.Width, ~group,
  1, 0.1, 1,
  1, 2.5, 1,
  7, 2.5, 1,
  7, 0.1, 1,
  1, 0.1, 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```


## 1.2 Hierarchical Clustering

Cluster sites according to species composition. Use the `dune` dataset from the vegan R package.

### 1.2.1 Load `dune` dataset
```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```
**Question: What are the rows and columns composed of in the dune data frame?**\
 - Columns are observations of 30 vegetative dune meadow species.\
 - Rows are the 20 sites where the species were observed.\

### 1.2.2 Calculate Ecological Distances on sites
Preparation of concepts with easier dataset `sites`
```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites

# Computation of Dissimilarity Indices for Community Ecologists
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan

sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean

sites_bray <- vegdist(sites, method="bray")
sites_bray
```

**Question:** In your own words, how does Bray Curtis differ from Euclidean distance?\
 - Euclidean distance is good for continuous numerical variables and reflects absolute distances.\
 - Bray-Curtisis best for categorical or binary data and similar to Euclidean, but it’s more appropriate when we want to differentiate profiles and also take relative magnitudes into account.
  
**Question:** Which function comes first, vegdist() or hclust(), and why? See HOMLR 21.3.1 Agglomerative hierarchical clustering.\
 - We first compute the dissimilarity values with vegdist() (i.e. the distance) because you need to measure those distances first before you assign them to a cluster.

### 1.2.4 Agglomerative hierarchical clustering on dune
#### Hierarchical cluster 1
```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)

as.matrix(d)[1:5, 1:5]

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)

```

#### Hierarchical cluster 2
```{R}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac

# Dendrogram plot of hc2
plot(hc2, which.plot = 2)

# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

**Question:** In your own words how does hclust() differ from agnes()? See HOMLR 21.3.1 Agglomerative hierarchical clustering and help documentation (?hclust(), ?agnes()).\
 - hclust() and agnes() are both similar in that they assign each object to its own cluster and then the algorithm proceeds iteratively, **at each stage joining the two most similar clusters, continuing until there is just a single cluster.** The main difference is that with agnes(), you also get the Agglomerative Coefficient (AC). The AC describes the strength of the clustering structure which is dependent upon the distance measure used.
  
#### Hierarchical cluster 3
```{R}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac

# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```

**Question:** Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?\
 - Ward's method identifies the "best" model in terms of AC. The Ward's method minimizes the total within-cluster variance. 
  
**Question:** In your own words how does agnes() differ from diana()?\
 - diana() is a divisive method and a top-down approach rather than agnes() which is an agglomerative technqiue that is bottomup. With diana(), there is no agglomeration method to input either.

### 1.2.5 Divisive hierarchical clustering on `dune`
```{r}
# DIvisive ANAlysis Clustering

# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc

plot(hc4, which.plot = 2)
```

### 1.2.6 Determining optimal clusters
 - read [HOLMR 21.4](https://bradleyboehmke.github.io/HOML/hierarchical.html#determining-optimal-clusters)
```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

**Question:** How do the optimal number of clusters compare between methods for those with a dashed line?\
 - There is typically no definitively clear optimal number of clusters. However, you can run multiple methods and compare to see what general statistic could be used. The dashed line in the example above recommends between 3-4 clusters.

### 1.2.7 Working with Dendrograms
 - See text to accompany code: [HOMLR 21.5 Working with dendrograms](https://bradleyboehmke.github.io/HOML/hierarchical.html#working-with-dendrograms).
```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)

# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")

```

**Question:** In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection?\
 - The biggest determinant of relatedness between observations is the height of the branch between an observation and the clusters of observations below them which indicates the distance between the observation and that cluster it is joined to.

# 2. Ordination

## Learning Objectives
In this lab, you will play with unsupervised classification techniques while working with ecological community datasets.
  
 - **Ordination** orders sites near each other based on similarity. It is a multivariate analysis technique used to effectively collapse dependent axes into fewer dimensions, i.e. dimensionality reduction.
    - **Principal Components Analyses (PCA)** is the most common and oldest technique that assumes linear relationships between axes. You will follow a non-ecological example from [Chapter 17 Principal Components Analysis | Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/pca.html) to learn about this commonly used technique.
    - **Non-metric MultiDimensional Scaling (NMDS)** allows for non-linear relationships. This ordination technique is implemented in the R package [vegan](https://cran.r-project.org/web/packages/vegan/index.html). You’ll use an ecological dataset, species and environment from lichen pastures that reindeer forage upon, with excerpts from the vegantutor vignette (source) to apply these techniques:
      - **Unconstrained ordination** on species using NMDS;
      - Overlay with environmental gradients; and
      - **Constrained ordination** on species and environment using another ordination technique, canonical correspondence analysis (CCA)
  

## 1.1 Principal Components Analysis (PCA)
### 1.1.1 Prerequisite
 -  - Prerequisite is to read Chapter 17 (linked above) in entirety.
```{r}
# load R packages
librarian::shelf(
  dplyr, ggplot2, h2o)

# set seed for reproducible results
set.seed(42)

# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)

my_basket
```

### Performing PCA in R

See supporting text: [17.4 Performing PCA in R](https://bradleyboehmke.github.io/HOML/pca.html#performing-pca-in-r)

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca

# viewing PCA1 eigenvectors. Values 
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()

# comparison between PCA1 and PCA2
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```
### Eigenvalue criterion

See supporting text: [17.5.1 Eigenvalue criterion](https://bradleyboehmke.github.io/HOML/pca.html#eigenvalue-criterion).


```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)

# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")

# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.9))

# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

**Question:** How many initial principal components are chosen with respect to dimensions of the input data?\
 - 42 PCAs.

**Question:** What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip)\
 - Bulmers, red wine, and fosters.
 
**Question:** What category of grocery items contribute the least to PC1 but positively towards PC2?\
 - vegetables (carrot, potato, broccoli)

**Question:** How many principal components would you include to explain 90% of the total variance?\
 - 36 principal components

**Question:** How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?\
 - 8 principal components

**Question:** What are a couple of disadvantages to using PCA?\
 - PCA can be highly affected by outliers.
 - PCA does not perform as well in very high dimensional space where complex nonlinear patterns often exist.

## Non-metric MultiDimensional Scaling (NMDS)

### Unconstrained Ordination on Species

See supporting text: **2.1 Non-metric Multidimensional scaling** in [vegantutor.pdf](https://github.com/bbest/eds232-ml/raw/main/files/vegantutor.pdf): 


```{r}
# load R packages
librarian::shelf(
  vegan, vegan3d)

# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)

# stress plot or shepard plot
stressplot(vare.mds0)

# ordination plot
ordiplot(vare.mds0, type = "t")

vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds

plot(vare.mds, type = "t")
```

**Question:** What are the dimensions of the varespec data frame and what do rows versus columns represent?\
 - The `varespec` data frame has 24 rows and 44 columns. Columns are estimated cover values of 44 species. The variable names are formed from the scientific names, and are self explanatory for anybody familiar with the vegetation type.
  
**Question:** The “stress” in a stressplot represents the difference between the observed input distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?\
 - The NMDS fit $R^2$ value of 0.99 is 0.047 units better than the linear fit.  This indicates less information is loss in dimensional reduction with NMDS vs linear fit models.\
 - Stress exists when the distances between points do not equal points in the matrix. As you expand axes (dimensions) to the full number of species, you have zero stress. When you reduce the axes, you increase the stress.
  
**Question:** What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?\
 - MDS1 most dissimilar: -5, 28\
 - MDS2 most dissimilar: 14, 21
  
**Question:** What is the basic difference between metaMDS and monoMDS()? See 2.1 Non-metric Multidimensional scaling of vegantutor.pdf.
 - Both are ordination techniques. With monoMDS, the sign, orientation, scale, and location of the axes are not defined. With metaMDS only the sign is not defined while the orientation, scale, and location of the axes are defined. 
  

### Overlay with Environment

See supporting text in [vegantutor.pdf](https://github.com/bbest/eds232-ml/raw/main/files/vegantutor.pdf): 
  * 3 Environmental interpretation
  * 3.1 Vector fitting
  * 3.2 Surface fitting

```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef

plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)


ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```
  
**Question:** What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?\
 - Aluminium (Al) and manganese (Mn).

**Question:** Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?\
 - NMDS1
  
**Question:** What is the difference between “constrained” versus “unconstrained” ordination within ecological context?\
 - In unconstrained ordination, you first find the major compositional variation, and then relate this variation to observed environmental variation. In constrained ordination you do not want to display all or even most of the compositional variation, but only the variation that can be explained by the used environmental variables (i.e. constraints).
  

### Constrained Ordination on Species and Environment

See supporting text in [vegantutor.pdf](https://github.com/bbest/eds232-ml/raw/main/files/vegantutor.pdf): 
  * 4 Constrained ordination
  * 4.1 Model specification
  
Technically, this uses another technique `cca`, or canonical correspondence analysis.

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca

# plot ordination
plot(vare.cca)

# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
  
if (interactive()){
  ordirgl(vare.cca)
}
```

**Question:** What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environmnent? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?\
 - Site 28 and 4 are most differentiated by CCA1. The strongest environmental vector for CCA1 is Aluminum (Al).
