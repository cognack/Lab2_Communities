---
title: "chap17"
author: "Steven Cognac"
date: "1/25/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(h2o)
```

Document based on Chapter 17: Principle Component Analysis by [Hands on Machine Learning in R](https://bradleyboehmke.github.io/HOML/pca.html)

# get data
```{r}
# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)

```
# initiating h20
```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```
# PCA on `my_basket` data
```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000
)

```
# Viewing results of PCA
```{r}
# glimpse(my_pca) # view all information in pca

# my_pca@model$importance   # model object's printed output

my_pca
```
# Viewing influence of PCA1
 - for PC1 we see that the largest contributing features are mostly adult beverages (and apparently eating candy bars, smoking, and playing the lottery are also associated with drinking!).
```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point() +
  labs(title = "Feature loadings illustrating the influence that each variable has on the 
       first principal component",
       y = "Features")
```

# Comparing PCA1 vs PCA2
 - In this plot, adult beverages (e.g., whiskey and wine) have a positive contribution to PC1 but have a smaller and negative contribution to PC2. This means that transactions that include purchases of adult beverages tend to have larger than average values for PC1 but smaller than average for PC2.

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text() +
  labs(title = "Feature contribution for principal components one and two")
```
# Selecting the # of PC's to use
  
A primary goal in PCA is dimension reduction (in this case, feature reduction)
 - In essence, we want to come out of PCA with fewer components than original features, and with the caveat that these components explain us as much variation as possible about our data.
 
There are three common approaches to help make this decision:
 1. Eigenvalue criterion
 2. Proportion of variance explained criterion
 3. Scree plot criterion

## Eigenvalue Criterion

The sum of the eigenvalues is equal to the number of variables entered into the PCA; however, the eigenvalues will range from greater than one to near zero. An **eigenvalue of 1 means that the principal component would explain about one variable’s worth of the variability**. The rationale for using the eigenvalue criterion is that each component should explain at least one variable’s worth of the variability, and therefore, the eigenvalue criterion states that only components with eigenvalues greater than 1 should be retained.
  
We can compute the eigenvalues easily and identify PCs where the sum of eigenvalues is greater than or equal to 1. Consequently, using this criteria would have us retain the first 10 PCs in `my_basket`
```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2

# Sum of all eigenvalues equals number of variables
sum(eigen)

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```


```{r}
eigen_df <- eigen %>% 
  pivot_longer(cols = starts_with("pc"), names_to = "PC") %>% 
  rename(eigenvalue = value)

# Make PC column factor level to keep in order
eigen_df$PC <- factor(eigen_df$PC, levels = eigen_df$PC)

# plot points
ggplot(eigen_df, aes(x = PC, y = eigenvalue)) +
  geom_point() +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) +
  geom_hline(yintercept = 1, col = "red", alpha = 0.5) +
  labs(title = "Eigenvalue criterion keeps all principal components where 
       the sum of the eigenvalues are above or equal to a value of 1")
```

## Proportion of Variance Explained (PVE) criterion 
 - PVE identifies the optimal number of PCs to keep based on the total variability that we would like to account for.
  
```{r}
# Extract and plot PVE and CVE
pve_cve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist()
) %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
pve_cve
```
  
The first PCt in our example explains 5.46% of the feature variability, and the second principal component explains 5.17%. Together, the first two PCs explain 10.63% of the variability. Thus, if an analyst desires to choose the number of PCs required to explain at least 75% of the variability in our original data then they would choose the first 27 components.

```{r}
# How many PCs required to explain at least 75% of total variability
# min(which(ve$CVE >= 0.75))

pve_cve$data
## output should be 27
```

## Scree Plot Criterion

A scree plot shows the eigenvalues or PVE for each individual PC. Most scree plots look broadly similar in shape, starting high on the left, falling rather quickly, and then flattening out at some point. This is because the first component usually explains much of the variability, the next few components explain a moderate amount, and the latter components only explain a small fraction of the overall variability. The scree plot criterion looks for the “elbow” in the curve and selects all components just before the line flattens out, which looks like eight in our example

```{r}
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()
) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

So how many PCs should we use in the my_basket example? The frank answer is that there is no one best method for determining how many components to use. In this case, differing criteria suggest to retain 8 (scree plot criterion), 10 (eigenvalue criterion), and 26 (based on a 75% of variance explained requirement) components. 
